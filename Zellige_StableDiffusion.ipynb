{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zellige Generation using Stable Diffusion Model"
      ],
      "metadata": {
        "id": "7iH0RG2wwKPU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24IKIn_HwD07"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "#   SECTION 1 — INSTALL LIBS\n",
        "# ---------------------------\n",
        "!pip install -q diffusers==0.30.0 transformers accelerate safetensors datasets torchvision\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, DDPMScheduler\n",
        "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
        "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import AutoTokenizer\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "device = \"cuda\"\n",
        "print(\"Using:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 2 — MOUNT DRIVE\n",
        "# ---------------------------\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# CHANGE DATASET PATH HERE\n",
        "DATASET_DIR = \"/content/drive/MyDrive/ZelligeDataset/train\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Zellige_LoRA\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "w-3nYJxLxCYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 3 — TRAIN CONFIG\n",
        "# ---------------------------\n",
        "class TrainConfig:\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "    train_data_dir = DATASET_DIR\n",
        "    output_dir = OUTPUT_DIR\n",
        "    image_size = 512\n",
        "    batch_size = 2\n",
        "    num_epochs = 10\n",
        "    lr = 1e-4\n",
        "    lr_warmup_steps = 200\n",
        "    gradient_accumulation_steps = 1\n",
        "    max_train_steps = None\n",
        "    lora_rank = 4\n",
        "\n",
        "config = TrainConfig()\n"
      ],
      "metadata": {
        "id": "BHSq46u-xGQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 4 — DATASET CLASS\n",
        "# ---------------------------\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config.image_size, config.image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "class ZelligeDataset(Dataset):\n",
        "    def __init__(self, folder, tokenizer, caption=\"Moroccan Zellige pattern, geometric mosaic\"):\n",
        "        self.paths = [\n",
        "            os.path.join(folder, f) for f in os.listdir(folder)\n",
        "            if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
        "        ]\n",
        "        self.tokenizer = tokenizer\n",
        "        self.caption = caption\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = transform(img)\n",
        "\n",
        "        text_inputs = self.tokenizer(\n",
        "            self.caption,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": img,\n",
        "            \"input_ids\": text_inputs.input_ids[0],\n",
        "        }\n",
        "\n",
        "# Load tokenizer and dataset\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_id, subfolder=\"tokenizer\")\n",
        "dataset = ZelligeDataset(config.train_data_dir, tokenizer)\n",
        "train_dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "print(\"Number of training images:\", len(dataset))\n"
      ],
      "metadata": {
        "id": "0v0fLaThxIng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 5 — LOAD SD MODEL\n",
        "# ---------------------------\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    config.model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "vae = pipe.vae\n",
        "unet = pipe.unet\n",
        "text_encoder = pipe.text_encoder\n",
        "noise_scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "# Freeze everything except LoRA\n",
        "vae.requires_grad_(False)\n",
        "unet.requires_grad_(False)\n",
        "text_encoder.requires_grad_(False)"
      ],
      "metadata": {
        "id": "yJM9BDRJxP0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 6 — ADD LORA LAYERS\n",
        "# ---------------------------\n",
        "def add_lora_attention(unet, rank=4):\n",
        "    loras = {}\n",
        "    for name, module in unet.named_modules():\n",
        "        if hasattr(module, \"set_processor\"):\n",
        "            hidden = module.to_q.in_features\n",
        "            loras[name] = LoRAAttnProcessor(hidden, rank=rank)\n",
        "    unet.set_attn_processor(loras)\n",
        "\n",
        "    # Collect parameters\n",
        "    params = []\n",
        "    for proc in unet.attn_processors.values():\n",
        "        params.extend(list(proc.parameters()))\n",
        "    return params\n",
        "\n",
        "lora_params = add_lora_attention(unet, rank=config.lora_rank)\n",
        "print(\"Trainable LoRA params:\", sum(p.numel() for p in lora_params))\n",
        "\n",
        "optimizer = torch.optim.AdamW(lora_params, lr=config.lr)"
      ],
      "metadata": {
        "id": "kFhTPr6JxS01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 7 — TRAINING SETUP\n",
        "# ---------------------------\n",
        "num_steps_per_epoch = len(train_dataloader)\n",
        "total_steps = config.num_epochs * num_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=config.lr_warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n"
      ],
      "metadata": {
        "id": "pkMl4ASpxVEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 8 — TRAIN LOOP\n",
        "# ---------------------------\n",
        "global_step = 0\n",
        "unet.train()\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{config.num_epochs} ---\")\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        images = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "        # Encode images into latents\n",
        "        with torch.no_grad():\n",
        "            latents = vae.encode(images).latent_dist.sample() * 0.18215\n",
        "\n",
        "        # Add noise\n",
        "        noise = torch.randn_like(latents)\n",
        "        timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (latents.size(0),), device=device)\n",
        "        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        # Text embeddings\n",
        "        with torch.no_grad():\n",
        "            enc = text_encoder(input_ids)[0]\n",
        "\n",
        "        # Predict noise\n",
        "        noise_pred = unet(noisy_latents, timesteps, enc).sample\n",
        "\n",
        "        # Loss\n",
        "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % 50 == 0:\n",
        "            print(f\"Step {global_step}/{total_steps} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ],
      "metadata": {
        "id": "sv5jmh4RxYDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "#  SECTION 9 — SAVE LORA\n",
        "# ---------------------------\n",
        "print(\"Saving LoRA...\")\n",
        "\n",
        "lora_state = {}\n",
        "for name, module in unet.attn_processors.items():\n",
        "    if isinstance(module, LoRAAttnProcessor):\n",
        "        for k, v in module.state_dict().items():\n",
        "            lora_state[f\"{name}.{k}\"] = v.cpu()\n",
        "\n",
        "save_path = os.path.join(OUTPUT_DIR, \"zellige_lora.safetensors\")\n",
        "save_file(lora_state, save_path)\n",
        "\n",
        "print(\"LoRA saved to:\", save_path)"
      ],
      "metadata": {
        "id": "4OHyDgPtxbIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}